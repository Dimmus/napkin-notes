## Best Docs to Date
- [API usage overview](https://github.com/openstax/napkin-notes/blob/master/kevin/160921_biglearnApis/api_usage.md)
- [BL deployment overview](https://github.com/openstax/napkin-notes/blob/master/kevin/BiglearnArchitectureDeployment.pdf)
- [Drew's presentation](https://docs.google.com/presentation/d/1qoPqBLD4XqOsIfcM6aJH7IaDQRsxxuA6QBLy4GIZy7w/edit#slide=id.p)
- [Kevin's SPARFA Guide](https://github.com/openstax/sparfa-sandbox/blob/klb_sgd/klb_sparfa_guide/sparfa_guide.pdf)

## SGD-based SPARFA

I 
[implemented](https://github.com/openstax/sparfa-sandbox/tree/klb_sgd/klb_refactor/sgd)
the 
[adadelta variant](https://arxiv.org/pdf/1212.5701.pdf)
of SGD for SPARFA
because it is 
[reportedly](http://cs231n.github.io/neural-networks-3/)
at least as good as any other variant
(and often outperforms them)
and is insensitive to hyperparameters
(like initial learning rate).

When I ran it on our data, though,
Adadelta bottomed out at a much higher cost
than other SGD methods.
It turned out that, 
while fairly insensitive to the initial learning rate,
Adadelta is quite sensitive to espsilon:

<img src="https://github.com/openstax/napkin-notes/blob/master/kevin/summaries/Adadelta%20Epsilon.png" alt="steep" height="200">

Once I fixed that, it started behaving as expected - performance improved by a factor of roughly 2-3.  Adadelta is the blue plot below:

<img src="https://github.com/openstax/napkin-notes/blob/master/kevin/summaries/SGD%20Comparison.png" alt="steep" width="500" height="500">

<img src="https://github.com/openstax/napkin-notes/blob/master/kevin/summaries/SGD%20Comparison%20(zoom).png" alt="steep" width="500" height="500">

## Preventing CLUE-based Cheating

I created a 
[writeup](https://github.com/openstax/napkin-notes/blob/master/kevin/170314_clueCheating.md)
describing how CLUE-based cheating can be prevented in the new `Biglearn`.
