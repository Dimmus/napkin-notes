## Best Docs to Date
- [API usage overview](https://github.com/openstax/napkin-notes/blob/master/kevin/160921_biglearnApis/api_usage.md)
- [BL deployment overview](https://github.com/openstax/napkin-notes/blob/master/kevin/BiglearnArchitectureDeployment.pdf)
- [Drew's presentation](https://docs.google.com/presentation/d/1qoPqBLD4XqOsIfcM6aJH7IaDQRsxxuA6QBLy4GIZy7w/edit#slide=id.p)

## SGD-based SPARFA

I reviewed a chunk of the theory behind stochastic gradient descent (SGD) 
and how it can be applied to the SPARFA matrix factorization problem.

Some resources I found especially helpful:
- this [overview](https://arxiv.org/pdf/1609.04747.pdf) of gradient descent algorithms
- this summary of [sanity checks](http://cs231n.github.io/neural-networks-3/) for gradient descent code

Of specific interest is an algorithm called `adadelta`,
which seems to outperform more basic SGD-based algorithms
and also automatically adjusts the learning rate 
without the need for an initial seed.
Note the difference between `adadelta` (beige) and vanilla `SGD` (red):

<img src="https://github.com/openstax/napkin-notes/blob/master/kevin/summaries/opt2.gif" alt="steep"  width="300" height="300">
<img src="https://github.com/openstax/napkin-notes/blob/master/kevin/summaries/opt1.gif" alt="saddle" width="300" height="300">

Two big questions still linger:
* What is the best way to sample our response data?
  * Currently, Drew is loading all of the data into memory and processing it.
  This is okay for now, but if we want to parallelize SGD-based SPARFA or if we get too much data this will begin to break down.  
  Ideally, each inner iteration should only require a small number (say 100) responses to be loaded for the question/learner of interest 
  (depending on whether we're updating `W`/`d` or `U`).  
  But this would require `NQ`/`NL` (number of questions/learners) queries per step.
* If each SPARFA inner interation will require a data query, will db access times dominate?
  * Computing the contribution for a single question/learner takes on the order of 1ms or less.  
  Database queries typically take on the order of 0.3ms, and we'd need two per question/learner.  
  It doesn't seem easy/practical to bundle the queries for multiple questions/learners into a single query 
  (which would alleviate the problem), 
  so we are left with the following options:
    * load all the data into memory and run SPARFA on a single machine
    * accept that steps will be slower than desired
    * change how we sample to accomodate fewer db accesses

I plan to discuss this with Drew on Tuesday.

So far, the SGD-based SPARFA implementation seems like it should be simpler than both FISTA-based SPARFA (the published version) and sample-based SPARFA.  The trick will be managing the schluffing around of data.

## Python Database Libraries

Regardless of how we decide to sample and how many db accesses we have per SPARFA step,
we will need to have python code access a database.
To that end, I've started re-learning [sqlalchemy](http://www.sqlalchemy.org/),
python's primary database library.

So far I've been able to:

* create a database
* create tables in that database
* create records in the tables

I haven't really tried to use any "advanced" features 
like foreign keys or uniqueness constraints yet;
what I have done is all really basic stuff.
It's also super slow and time-consuming.

The big questions that remain here are:
* How will we handle migrations?
  * Python has a library called [alembic](http://alembic.zzzcomputing.com/en/latest/) which is supposed to be good for this...
  * It's also possible that we let Rails handle this, if we end up with Rails on these servers...
