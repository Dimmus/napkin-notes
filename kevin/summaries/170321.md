## Best Docs to Date
- [API usage overview](https://github.com/openstax/napkin-notes/blob/master/kevin/160921_biglearnApis/api_usage.md)
- [BL deployment overview](https://github.com/openstax/napkin-notes/blob/master/kevin/BiglearnArchitectureDeployment.pdf)
- [Drew's presentation](https://docs.google.com/presentation/d/1qoPqBLD4XqOsIfcM6aJH7IaDQRsxxuA6QBLy4GIZy7w/edit#slide=id.p)
- [Kevin's SPARFA Guide](https://github.com/openstax/sparfa-sandbox/blob/klb_sgd/klb_sparfa_guide/sparfa_guide.pdf)

## SGD-based SPARFA

### Imputation-based Termination

I
[implemented](https://github.com/openstax/sparfa-sandbox/blob/klb_sgd/klb_refactor/tests/test_sgd.py#L105-L392)
code to use 
[imputation](https://en.wikipedia.org/wiki/Imputation_(statistics)#Multiple_imputation)
accuracy as the iteration stopping criterion.
This, along with the 
[adadelta](https://arxiv.org/pdf/1212.5701.pdf)-based 
SGD implementation,
has reduced SPARFA times on both Physics and Bio
to less than 10 seconds.

The following figures show why this is the case.
Despite the ever-increasing improvement in prediction accuracy on the training data,
the accuracy on unseen imputation data peaks fairly quickly,
and then declines slightly.
This indicates that SPARFA is being over-trained to no avail:

<img src="https://github.com/openstax/napkin-notes/blob/master/kevin/summaries/TrainingImputation.png" alt="training_imputation" width="300" height="300">
<img src="https://github.com/openstax/napkin-notes/blob/master/kevin/summaries/TrainingImputationZoom.png" alt="training_imputation_zoom" width="300" height="300">

The current termination algorithm
tracks the current-best `W`, `d` matrices
in terms of imputation accuracy.
If the results do not improve
within some fixed time
(currently 5 sec)
SPARFA is stopped
and the best `W`, `d` matrices are returned.

Each of these plots shows 10 separate imputation runs,
and the resulting ~1% variation in accuracy across runs.

### Some Cause for Concern?

The following plots show
the variation (min, max, median) in `W` matrix entries
across imputation runs.
The fact that this variation is quite large,
especially for "smaller"-valued entries,
could potentially result
in significant variation in CLUEs
(though this has not yet been confirmed).
Drew and I agree that this should be investigated further.

<img src="https://github.com/openstax/napkin-notes/blob/master/kevin/summaries/W_Variation.png" alt="training_imputation" width="300" height="300">
<img src="https://github.com/openstax/napkin-notes/blob/master/kevin/summaries/W_VariationZoom1.png" alt="training_imputation_zoom" width="300" height="300">
<img src="https://github.com/openstax/napkin-notes/blob/master/kevin/summaries/W_VariationZoom2.png" alt="training_imputation_zoom" width="300" height="300">
