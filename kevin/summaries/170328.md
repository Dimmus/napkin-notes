## Best Docs to Date
- [API usage overview](https://github.com/openstax/napkin-notes/blob/master/kevin/160921_biglearnApis/api_usage.md)
- [BL deployment overview](https://github.com/openstax/napkin-notes/blob/master/kevin/BiglearnArchitectureDeployment.pdf)
- [Drew's presentation](https://docs.google.com/presentation/d/1qoPqBLD4XqOsIfcM6aJH7IaDQRsxxuA6QBLy4GIZy7w/edit#slide=id.p)
- [Kevin's SPARFA Guide](https://github.com/openstax/sparfa-sandbox/blob/klb_sgd/klb_sparfa_guide/sparfa_guide.pdf)

## SGD-based SPARFA

### Effect of Allowing SPARFA to Learn

The following plot shows the effects
of L1 and L2 regularizer penalties
(which control how much SPARFA is allowed to learn from data
versus sticking with the given concept hints)
on imputation performance.
Cost function 1 (CF1) has modest penalties,
whereas cost function 2 (CF2) has larger penalties.
Peak imputation accuracy imporoves by ~1%
when SPARFA is allowed to learn from the data:

<img src="https://github.com/openstax/napkin-notes/blob/master/kevin/summaries/CF_LearningComp.png" alt="learning effects" width="300" height="300">

## CLUE Calculations

I reviewed the CLUE calculation code,
and the derivation of Fisher Information
([this paper](http://people.missouristate.edu/songfengzheng/Teaching/MTH541/Lecture%20notes/Fisher_info.pdf) has a concise summary).

I believe we can improve the current implementation
by eliminating special cases that avoid divide-by-zero
by just adding a lower bound the the Fisher Information values.
This will allow us to use the "standard" formula
instead of the approximation we're currently using.

I am also close to assessing the variation in CLUE value
caused by imputation during SPARFA training.
